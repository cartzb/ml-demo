{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "257bf5f1",
   "metadata": {},
   "source": [
    "# 前馈神经网络 (Feedforward Neural Network) - 详细解释与实现\n",
    "\n",
    "这个Notebook展示了如何从头开始实现一个简单的前馈神经网络，并详细解释了其中涉及的每个步骤、数学原理以及它们在代码中的作用。\n",
    "\n",
    "我们将以一个经典的XOR问题为例，演示如何训练一个前馈神经网络来学习这种非线性映射。最后，我们还将使用可视化工具绘制神经网络的学习过程以及各层的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eafb24",
   "metadata": {},
   "source": [
    "## 1. 导入必要的库\n",
    "\n",
    "我们首先导入 `numpy` 用于矩阵运算，并导入 `matplotlib` 用于数据可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30153c1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T02:41:16.897367Z",
     "start_time": "2024-08-29T02:41:16.892920Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671983a9",
   "metadata": {},
   "source": [
    "## 2. 定义激活函数及其导数\n",
    "\n",
    "在神经网络中，激活函数用于将线性输入映射到非线性输出。我们使用 `sigmoid` 函数作为激活函数，并计算它的导数以便在反向传播时使用。\n",
    "\n",
    "### Sigmoid函数\n",
    "Sigmoid函数的数学表达式为：\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "这个函数将任意实数输入映射到 (0, 1) 之间，这对于概率估计非常有用。\n",
    "\n",
    "### Sigmoid导数\n",
    "为了在反向传播时更新权重，我们需要计算激活函数的导数。Sigmoid函数的导数表达式为：\n",
    "\n",
    "$$\n",
    "\\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85d26ac7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T02:41:16.913197Z",
     "start_time": "2024-08-29T02:41:16.906360Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义激活函数及其导数\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid 激活函数，将输入映射到 (0, 1) 之间\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Sigmoid 函数的导数，用于计算反向传播中的梯度\"\"\"\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f96b5c1",
   "metadata": {},
   "source": [
    "## 3. 定义前馈神经网络类\n",
    "\n",
    "我们将创建一个 `NeuralNetwork` 类，其中包含神经网络的初始化、前向传播、反向传播和训练函数。\n",
    "\n",
    "### 前向传播 (Feedforward)\n",
    "前向传播的目的是通过网络计算输出。我们通过以下步骤实现：\n",
    "\n",
    "1. **隐藏层输入计算**：输入层的数据与隐藏层的权重相乘，并加上偏置得到隐藏层的输入。\n",
    "   $$\n",
    "   z^{(1)} = X \\cdot W^{(1)} + b^{(1)}\n",
    "   $$\n",
    "\n",
    "2. **隐藏层输出计算**：通过激活函数处理隐藏层的输入，得到隐藏层的输出。\n",
    "   $$\n",
    "   a^{(1)} = \\sigma(z^{(1)})\n",
    "   $$\n",
    "\n",
    "3. **输出层输入计算**：隐藏层的输出与输出层的权重相乘，并加上偏置得到输出层的输入。\n",
    "   $$\n",
    "   z^{(2)} = a^{(1)} \\cdot W^{(2)} + b^{(2)}\n",
    "   $$\n",
    "\n",
    "4. **输出层输出计算**：通过激活函数处理输出层的输入，得到最终的预测输出。\n",
    "   $$\n",
    "   a^{(2)} = \\sigma(z^{(2)})\n",
    "   $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d95033a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T02:41:16.936956Z",
     "start_time": "2024-08-29T02:41:16.930674Z"
    }
   },
   "outputs": [],
   "source": [
    "# 前馈神经网络类\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # 初始化权重和偏置，随机小值初始化\n",
    "        self.weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
    "        self.weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
    "        self.bias_hidden = np.random.randn(hidden_size)\n",
    "        self.bias_output = np.random.randn(output_size)\n",
    "        \n",
    "        # 保存误差历史记录，便于绘制误差曲线\n",
    "        self.error_history = []\n",
    "\n",
    "    def feedforward(self, X):\n",
    "        \"\"\"前向传播：从输入层 -> 隐藏层 -> 输出层\"\"\"\n",
    "        # 输入层到隐藏层的线性组合\n",
    "        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
    "        # 隐藏层输出，通过激活函数处理\n",
    "        self.hidden_output = sigmoid(self.hidden_input)\n",
    "        \n",
    "        # 隐藏层到输出层的线性组合\n",
    "        self.final_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
    "        # 最终输出\n",
    "        self.final_output = sigmoid(self.final_input)\n",
    "        \n",
    "        return self.final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7645035",
   "metadata": {},
   "source": [
    "### 反向传播 (Backpropagation)\n",
    "反向传播通过计算输出误差，并通过网络的反向传播来更新权重和偏置。我们通过以下步骤实现：\n",
    "\n",
    "1. **输出层误差计算**：计算输出层的误差，即期望输出与实际输出的差值。\n",
    "   $$\n",
    "   \\delta^{(2)} = (y - a^{(2)}) \\cdot \\sigma'(z^{(2)})\n",
    "   $$\n",
    "\n",
    "2. **隐藏层误差计算**：利用输出层的误差反向传播到隐藏层，计算隐藏层的误差。\n",
    "   $$\n",
    "   \\delta^{(1)} = \\delta^{(2)} \\cdot W^{(2)T} \\cdot \\sigma'(z^{(1)})\n",
    "   $$\n",
    "\n",
    "3. **更新权重和偏置**：根据误差的梯度更新权重和偏置，使模型的预测更接近期望输出。\n",
    "   $$\n",
    "   W^{(2)} \\leftarrow W^{(2)} + \\eta \\cdot a^{(1)T} \\cdot \\delta^{(2)}\n",
    "   $$\n",
    "   $$\n",
    "   W^{(1)} \\leftarrow W^{(1)} + \\eta \\cdot X^T \\cdot \\delta^{(1)}\n",
    "   $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcaee30c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T02:41:16.945251Z",
     "start_time": "2024-08-29T02:41:16.938067Z"
    }
   },
   "outputs": [],
   "source": [
    "    def backpropagation(self, X, y, learning_rate):\n",
    "        \"\"\"反向传播：根据误差调整权重和偏置\"\"\"\n",
    "        # 前向传播获取输出\n",
    "        output = self.feedforward(X)\n",
    "        \n",
    "        # 计算输出层误差\n",
    "        error = y - output\n",
    "        self.error_history.append(np.mean(np.abs(error)))  # 记录误差\n",
    "        \n",
    "        # 计算输出层到隐藏层的梯度\n",
    "        d_output = error * sigmoid_derivative(output)\n",
    "        \n",
    "        # 计算隐藏层误差\n",
    "        error_hidden = d_output.dot(self.weights_hidden_output.T)\n",
    "        d_hidden = error_hidden * sigmoid_derivative(self.hidden_output)\n",
    "        \n",
    "        # 更新权重和偏置\n",
    "        self.weights_hidden_output += self.hidden_output.T.dot(d_output) * learning_rate\n",
    "        self.weights_input_hidden += X.T.dot(d_hidden) * learning_rate\n",
    "        self.bias_output += np.sum(d_output, axis=0) * learning_rate\n",
    "        self.bias_hidden += np.sum(d_hidden, axis=0) * learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42268975",
   "metadata": {},
   "source": [
    "### 训练 (Training)\n",
    "训练神经网络的过程是通过多次迭代前向传播和反向传播，不断优化网络的权重和偏置，以降低输出误差。\n",
    "\n",
    "我们可以通过 `train` 函数指定训练轮数（epochs）和学习率（learning rate），在训练过程中，误差会逐渐减少。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba7dcb0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T02:41:16.951836Z",
     "start_time": "2024-08-29T02:41:16.946263Z"
    }
   },
   "outputs": [],
   "source": [
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        \"\"\"训练神经网络：通过多次迭代优化权重和偏置\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            self.backpropagation(X, y, learning_rate)\n",
    "            # 可视化训练过程中的误差\n",
    "            if epoch % 1000 == 0:\n",
    "                print(f'Epoch {epoch}, Error: {self.error_history[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cc087c-95bd-4ec2-a99e-bb5790743609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
